{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Training.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"nN9nyNyTS11r"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"DxiB8MQuSxf1"},"outputs":[],"source":["!pip install -U cade\n","!pip install git+https://github.com/valedica/gensim.git"]},{"cell_type":"code","source":["!pip install gensim"],"metadata":{"id":"VTom8mbRS4aS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from gensim.models import Word2Vec\n","from collections import Counter\n","import random\n","from cade.cade import CADE\n","import string"],"metadata":{"id":"3tjQJzlqS9Sd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"TjfuZvYZS-_L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#os.chdir('/content/drive/MyDrive/Magistrale/Secondo semestre/DS/Progetto/')"],"metadata":{"id":"ikQr1tAbTAo1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Functions "],"metadata":{"id":"F1PP6EjUTDmF"}},{"cell_type":"code","source":["def getsentences_W2V(file_name):\n","  #os.chdir('/content/drive/MyDrive/Magistrale/Secondo semestre/DS/Progetto/Sentences_nl')\n","  sentences = []\n","\n","  with open(file_name + '_nl.txt', 'r') as fp:\n","      for line in fp:\n","          x = line[1:-2]\n","          x = x.replace('\\'','')\n","          x = x.replace(' ', '')\n","          x = x.split(',')\n","\n","          sentences.append(x)\n","          \n","  return sentences"],"metadata":{"id":"Zvx2TpfOTHUk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def getsentences_CADE(filename):\n","  #os.chdir('/content/drive/MyDrive/Magistrale/Secondo semestre/DS/Progetto/Sentences_nl')\n","  sentences = ''\n","\n","  with open(filename + '.txt', 'r') as fp:\n","      for line in fp:\n","          line = line.replace('[', '')\n","          line = line.replace(']', '')\n","          line = line.replace('\\'','')\n","          line = line.replace(',', '')\n","          line = line.replace('\\n', ' ')\n","\n","          sentences += line\n","\n","  with open(filename + '_nl_cade.txt', 'w') as fp:\n","      fp.write(sentences)"],"metadata":{"id":"J6266IyOTmBW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def training_W2V(sentences, holytext):\n","  #os.chdir('/content/drive/MyDrive/Magistrale/Secondo semestre/DS/Progetto/' + holytext + '_nl_W2V_embeddings')\n","  #train 30 models and save them\n","\n","  for it in range(30):\n","    model = Word2Vec(sentences = sentences,\n","                    #window = 5, default value\n","                    min_count=10, #not consider word with absolute frequency <10 \n","                    size=300, #vector size \n","                    sg = 1, #skipgram algorithm\n","                    hs = 0,\n","                    negative = 5, #negative sampling with 5 noise words\n","                    workers = 5, #faster process\n","                    iter = 6 #6 iterations\n","                    )\n","  \n","    model.save(holytext.lower() + \"_\" + str(it) + \".model\")"],"metadata":{"id":"9ehYM6X2TOqR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def training_CADE(holytext, religion):\n","  #os.chdir('/content/drive/MyDrive/Magistrale/Secondo semestre/DS/Progetto/' + holytext + '_nl_CADE_embeddings')\n","\n","  aligner = CADE(min_count=10,  \n","                  size=300,\n","                  sg = 1, \n","                  #hs = 0,\n","                  ns = 5, \n","                  workers = 5,\n","                  siter = 6)\n","\n","  for it in range(30):\n","    aligner.train_compass('compass.txt', overwrite=True)\n","\n","    os.rename(religion + '_sentences_nl_cade.txt', holytext + '_nl_cade'+str(it)+'.txt')\n","\n","    slice_model = aligner.train_slice(holytext + '_nl_cade'+str(it)+'.txt', save=True)\n","\n","    os.rename(holytext + '_nl_cade'+str(it)+'.txt', religion + '_sentences_nl_cade.txt')"],"metadata":{"id":"upk4uo1uUKnF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Introduction\n","We want to perform a semantic analysis of the corpora collected using word2vec embeddings. Since the main focus of our work is the exploration and comparison of the obtained embeddings we have to worry about the stability of this ones. \n","Infact, due to their stochastic component, word2vec embeddings are not stable. For example, the most similar words to a given word could change between models even though the models are trained on the same corpora.\n","\n","With the goal of increasing the stability of the models obtained we take the following decisions. We decide to use the Skip Gram method because it seems to work better than other methods on semantic tasks(Mikolov et al., 2013), like ours. We decide to use the Skip Gram Negative Sampling with 5 noise words because it seems to be more stable than the Skip Gram Hierarchical Softmax (Hellrich&Hann, 2016). We perform 6 iterations over the corpora for each embedding because it seems to be a good trade off between computational cost and stability obtained (Hellrich&Hann, 2016). We set a context window of 5, a minimum frequency of 10 and a vector size of 300, considering these to be commonly used values.\n","\n","In addition, to increase the significance of our conclusions we train 30 embeddings for each corpora instead of one and we perform the analysis combining the results (Martina Schories, 2020).\n","\n","We use Word2vec to create models to explore each corpora individually. We use CADE to align corpora and create other Word2vec models for comparison of corpora. We use the same parameters already described in both cases.\n"],"metadata":{"id":"Q3X2hfChXWxT"}},{"cell_type":"markdown","source":["# Word2vec - training "],"metadata":{"id":"3Mcyz7rsTVYo"}},{"cell_type":"code","source":["rel_dict = {'Christian_sentences' : 'Bible', 'Islam_sentences' : 'Quran', 'Hinduism_sentences' : 'VedasUpanishads', 'Buddhism_sentences' : 'Tripitaka'}\n","\n","for text in rel_dict.keys():\n","  sentences = getsentences_W2V(text)\n","  training_W2V(sentences, rel_dict[text])"],"metadata":{"id":"oZx1BaX_TdvB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# CADE - training"],"metadata":{"id":"fRzv0QLXVDEm"}},{"cell_type":"code","source":["for text in rel_dict.keys():\n","  sentences = getsentences_CADE(text)"],"metadata":{"id":"1buaURNlVcKg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#os.chdir('/content/drive/MyDrive/Magistrale/Secondo semestre/DS/Progetto/' + holytext + '_nl_CADE_embeddings')"],"metadata":{"id":"KP4err3XVvjm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cat Christian_sentences_nl_cade.txt Islam_sentences_nl_cade.txt Hinduism_sentences_nl_cade.txt Buddhism_sentences_nl_cade.txt > compass.txt"],"metadata":{"id":"rnijqe8CVgec"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for text in rel_dict.keys():\n","   training_CADE(sentences, rel_dict[text])"],"metadata":{"id":"1OOf3w9nVCgs"},"execution_count":null,"outputs":[]}]}